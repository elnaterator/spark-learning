{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYSPARK_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYTHONPATH=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/pyspark.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/*.zip:\n",
      "SPARK_HOME=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3\n",
      "PYTHONUNBUFFERED=1\n",
      "PYTHONIOENCODING=utf-8\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n"
     ]
    }
   ],
   "source": [
    "# Check env vars\n",
    "!env | grep -e \"SPARK\" -e \"PYTHON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/27 11:04:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.150:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>learn_dataframes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1045a0220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "            SparkSession.builder.appName(\"learn_dataframes\")\n",
    "                .master(\"local[4]\")\n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yc/s3k6lhps2dx6f_lyhxcddtqm0000gp/T/ipykernel_77209/605480908.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add scrollbars to data for display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD of employees\n",
    "data = [\n",
    "    [1, \"John\", 10000],\n",
    "    [2, \"Jane\", 15000],\n",
    "    [3, \"Joe\", 5000],\n",
    "    [4, \"Mary\", 20000],\n",
    "    [5, \"Mike\", 25000]\n",
    "]\n",
    "\n",
    "employeesRdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  1|John| 10000|\n",
      "|  2|Jane| 15000|\n",
      "|  3| Joe|  5000|\n",
      "|  4|Mary| 20000|\n",
      "|  5|Mike| 25000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame and show content\n",
    "employeesDf = employeesRdd.toDF([\"id\", \"name\", \"salary\"])\n",
    "\n",
    "employeesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame schema\n",
    "employeesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|salary|\n",
      "+---+----+------+\n",
      "|  1|John| 10000|\n",
      "|  2|Jane| 15000|\n",
      "|  3| Joe|  5000|\n",
      "|  4|Mary| 20000|\n",
      "|  5|Mike| 25000|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creat dataframe from collection directly\n",
    "employeesRdd = spark.createDataFrame(data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "employeesRdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| Id| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| John| 10000|\n",
      "|  2|Smith| 20000|\n",
      "|  3| Mark| 30000|\n",
      "|  4|David| 40000|\n",
      "|  5| Paul| 50000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create dataframe from a file\n",
    "employeesDf = spark.read.csv(\"data/employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "employeesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "employeesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| Id| Name|Salary|\n",
      "+---+-----+------+\n",
      "|  1| John| 10000|\n",
      "|  2|Smith| 20000|\n",
      "|  3| Mark| 30000|\n",
      "|  4|David| 40000|\n",
      "|  5| Paul| 50000|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a tab separated file\n",
    "employeesDf = spark.read.csv(\"data/employees.tsv\", header=True, inferSchema=True, sep=\"\\t\")\n",
    "\n",
    "employeesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "| id| name| salary|\n",
      "+---+-----+-------+\n",
      "|  1| John|10000.0|\n",
      "|  2|Smith|20000.0|\n",
      "|  3| Mark|30000.0|\n",
      "|  4|David|40000.0|\n",
      "|  5| Paul|50000.0|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema manually\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "employeesDf = spark.read.csv(\"data/employees.csv\", header=True, schema=schema)\n",
    "\n",
    "employeesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------------------+------+--------+--------------------+\n",
      "| distance|  duration|           end_point|  fare|route_id|         start_point|\n",
      "+---------+----------+--------------------+------+--------+--------------------+\n",
      "|5.6 miles|20 minutes|{40.768437, -73.9...|$15.00|       1|{40.712776, -74.0...|\n",
      "|0.7 miles| 5 minutes|{40.748817, -73.9...| $4.00|       2|{40.758896, -73.9...|\n",
      "|0.0 miles| 0 minutes|{40.748817, -73.9...| $2.50|       3|{40.748817, -73.9...|\n",
      "|0.0 miles| 0 minutes|{40.748817, -73.9...| $2.50|       4|{40.748817, -73.9...|\n",
      "|0.0 miles| 0 minutes|{40.748817, -73.9...| $2.50|       5|{40.748817, -73.9...|\n",
      "|0.0 miles| 0 minutes|{40.748817, -73.9...| $2.50|       6|{40.748817, -73.9...|\n",
      "|0.0 miles| 0 minutes|{40.748817, -73.9...| $2.50|       7|{40.748817, -73.9...|\n",
      "+---------+----------+--------------------+------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a multiline json file\n",
    "routesDf = spark.read.json(\"data/taxi_routes.json\", multiLine=True)\n",
    "\n",
    "routesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- distance: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- end_point: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- fare: string (nullable = true)\n",
      " |-- route_id: string (nullable = true)\n",
      " |-- start_point: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "routesDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------+----------+------+\n",
      "|route_id|         start_point|           end_point| distance|  duration|  fare|\n",
      "+--------+--------------------+--------------------+---------+----------+------+\n",
      "|       1|{40.712776, -74.0...|{40.768437, -73.9...|5.6 miles|20 minutes|$15.00|\n",
      "|       2|{40.758896, -73.9...|{40.748817, -73.9...|0.7 miles| 5 minutes| $4.00|\n",
      "|       3|{40.748817, -73.9...|{40.748817, -73.9...|0.0 miles| 0 minutes| $2.50|\n",
      "|       4|{40.748817, -73.9...|{40.748817, -73.9...|0.0 miles| 0 minutes| $2.50|\n",
      "|       5|{40.748817, -73.9...|{40.748817, -73.9...|0.0 miles| 0 minutes| $2.50|\n",
      "|       6|{40.748817, -73.9...|{40.748817, -73.9...|0.0 miles| 0 minutes| $2.50|\n",
      "|       7|{40.748817, -73.9...|{40.748817, -73.9...|0.0 miles| 0 minutes| $2.50|\n",
      "+--------+--------------------+--------------------+---------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define our own schema for the json and read file using that\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, StructType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"route_id\", StringType(), True),\n",
    "    StructField(\"start_point\", StructType([\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"end_point\", StructType([\n",
    "        StructField(\"latitude\", DoubleType(), True),\n",
    "        StructField(\"longitude\", DoubleType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"distance\", StringType(), True),\n",
    "    StructField(\"duration\", StringType(), True),\n",
    "    StructField(\"fare\", StringType(), True)\n",
    "])\n",
    "\n",
    "taxiRoutesDf = spark.read.json(\"data/taxi_routes.json\", multiLine=True, schema=schema)\n",
    "\n",
    "taxiRoutesDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- route_id: string (nullable = true)\n",
      " |-- start_point: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- end_point: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- fare: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print schema\n",
    "taxiRoutesDf.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
