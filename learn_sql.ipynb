{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYSPARK_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYTHONPATH=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/pyspark.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/*.zip:\n",
      "SPARK_HOME=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3\n",
      "PYTHONUNBUFFERED=1\n",
      "PYTHONIOENCODING=utf-8\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n"
     ]
    }
   ],
   "source": [
    "# Check env vars\n",
    "!env | grep -e \"SPARK\" -e \"PYTHON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add scrollbars to data for display\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/27 15:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.150:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>learn_dataframes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x106758550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "            SparkSession.builder.appName(\"learn_dataframes\")\n",
    "                .master(\"local[4]\")\n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------+-------+----------+\n",
      "|         City|State|Country|ZipCode|Population|\n",
      "+-------------+-----+-------+-------+----------+\n",
      "|      Seattle|   WA|    USA|  98101|    652405|\n",
      "|   Bellingham|   WA|    USA|  98225|     82235|\n",
      "|     Portland|   OR|    USA|  97201|    609456|\n",
      "|       Eugene|   OR|    USA|  97401|    221452|\n",
      "|San Francisco|   CA|    USA|  94101|    837442|\n",
      "|  Los Angeles|   CA|    USA|  90001|   3884307|\n",
      "+-------------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a csv file into a dataframe using a sql schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "schema = \"City STRING, State STRING, Country STRING, ZipCode LONG, Population INT\"\n",
    "\n",
    "citiesDF = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", False)\n",
    "    .schema(schema)\n",
    "    .load(\"data/cities.csv\")\n",
    ")\n",
    "\n",
    "citiesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+----------+\n",
      "|      City|State|Country|ZipCode|Population|\n",
      "+----------+-----+-------+-------+----------+\n",
      "|   Seattle|   WA|    USA|  98101|    652405|\n",
      "|Bellingham|   WA|    USA|  98225|     82235|\n",
      "+----------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only WA cities using sql syntax\n",
    "citiesDF.createOrReplaceTempView(\"cities\")\n",
    "\n",
    "citiesWA = spark.sql(\"SELECT * FROM cities WHERE State = 'WA'\")\n",
    "\n",
    "citiesWA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+----------+\n",
      "|      City|State|Country|ZipCode|Population|\n",
      "+----------+-----+-------+-------+----------+\n",
      "|   Seattle|   WA|    USA|  98101|    652405|\n",
      "|Bellingham|   WA|    USA|  98225|     82235|\n",
      "+----------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save this data to a managed table\n",
    "citiesWA.write.saveAsTable(\"citieswa\")\n",
    "\n",
    "# Select from the managed table\n",
    "spark.sql(\"SELECT * FROM citieswa\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                            |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                               |null   |\n",
      "|State                       |string                                                                               |null   |\n",
      "|Country                     |string                                                                               |null   |\n",
      "|ZipCode                     |bigint                                                                               |null   |\n",
      "|Population                  |int                                                                                  |null   |\n",
      "|                            |                                                                                     |       |\n",
      "|# Detailed Table Information|                                                                                     |       |\n",
      "|Database                    |default                                                                              |       |\n",
      "|Table                       |citieswa                                                                             |       |\n",
      "|Created Time                |Wed Dec 27 15:21:35 PST 2023                                                         |       |\n",
      "|Last Access                 |UNKNOWN                                                                              |       |\n",
      "|Created By                  |Spark 3.3.4                                                                          |       |\n",
      "|Type                        |MANAGED                                                                              |       |\n",
      "|Provider                    |parquet                                                                              |       |\n",
      "|Location                    |file:/Users/c11309a/Workspace/github-personal/spark-learning/spark-warehouse/citieswa|       |\n",
      "+----------------------------+-------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED citieswa\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+----------+\n",
      "|      City|State|Country|ZipCode|Population|\n",
      "+----------+-----+-------+-------+----------+\n",
      "|   Seattle|   WA|    USA|  98101|    652405|\n",
      "|Bellingham|   WA|    USA|  98225|     82235|\n",
      "+----------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save this data to an unmanaged table by specifying the data location\n",
    "import os\n",
    "\n",
    "(\n",
    "    citiesWA.write\n",
    "        .option(\"path\", f\"{os.getcwd()}/output/citieswa\")\n",
    "        .option(\"format\", \"parquet\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"citieswaunmanaged\")\n",
    ")\n",
    "\n",
    "# Select from the unmanaged table\n",
    "spark.sql(\"SELECT * FROM citieswaunmanaged\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                     |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                        |null   |\n",
      "|State                       |string                                                                        |null   |\n",
      "|Country                     |string                                                                        |null   |\n",
      "|ZipCode                     |bigint                                                                        |null   |\n",
      "|Population                  |int                                                                           |null   |\n",
      "|                            |                                                                              |       |\n",
      "|# Detailed Table Information|                                                                              |       |\n",
      "|Database                    |default                                                                       |       |\n",
      "|Table                       |citieswaunmanaged                                                             |       |\n",
      "|Created Time                |Wed Dec 27 15:21:37 PST 2023                                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                                       |       |\n",
      "|Created By                  |Spark 3.3.4                                                                   |       |\n",
      "|Type                        |EXTERNAL                                                                      |       |\n",
      "|Provider                    |parquet                                                                       |       |\n",
      "|Location                    |file:///Users/c11309a/Workspace/github-personal/spark-learning/output/citieswa|       |\n",
      "|Storage Properties          |[format=parquet]                                                              |       |\n",
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED citieswaunmanaged\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop both tables, note that the underlying data for the managed table is also deleted, but not for the unmanaged table\n",
    "spark.sql(\"DROP TABLE citieswa\")\n",
    "spark.sql(\"DROP TABLE citieswaunmanaged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the table using sql\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE citieswa\n",
    "USING parquet\n",
    "LOCATION \"{os.getcwd()}/output/citieswa\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                     |comment|\n",
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "|City                        |string                                                                        |null   |\n",
      "|State                       |string                                                                        |null   |\n",
      "|Country                     |string                                                                        |null   |\n",
      "|ZipCode                     |bigint                                                                        |null   |\n",
      "|Population                  |int                                                                           |null   |\n",
      "|                            |                                                                              |       |\n",
      "|# Detailed Table Information|                                                                              |       |\n",
      "|Database                    |default                                                                       |       |\n",
      "|Table                       |citieswa                                                                      |       |\n",
      "|Created Time                |Wed Dec 27 15:21:37 PST 2023                                                  |       |\n",
      "|Last Access                 |UNKNOWN                                                                       |       |\n",
      "|Created By                  |Spark 3.3.4                                                                   |       |\n",
      "|Type                        |EXTERNAL                                                                      |       |\n",
      "|Provider                    |parquet                                                                       |       |\n",
      "|Location                    |file:///Users/c11309a/Workspace/github-personal/spark-learning/output/citieswa|       |\n",
      "+----------------------------+------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Describe the table\n",
    "spark.sql(\"DESCRIBE TABLE EXTENDED citieswa\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+----------+\n",
      "|City      |State|Country|ZipCode|Population|\n",
      "+----------+-----+-------+-------+----------+\n",
      "|Seattle   |WA   |USA    |98101  |652405    |\n",
      "|Bellingham|WA   |USA    |98225  |82235     |\n",
      "+----------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select from the table\n",
    "spark.sql(\"SELECT * FROM citieswa\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add data from citiesWA data frame to the table\n",
    "citiesWA.createOrReplaceTempView(\"citieswa_view\")\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO table citieswa SELECT * FROM citieswa_view\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+-------+----------+\n",
      "|City      |State|Country|ZipCode|Population|\n",
      "+----------+-----+-------+-------+----------+\n",
      "|Seattle   |WA   |USA    |98101  |652405    |\n",
      "|Bellingham|WA   |USA    |98225  |82235     |\n",
      "|Seattle   |WA   |USA    |98101  |652405    |\n",
      "|Bellingham|WA   |USA    |98225  |82235     |\n",
      "+----------+-----+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select from the table\n",
    "spark.sql(\"SELECT * FROM citieswa\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
