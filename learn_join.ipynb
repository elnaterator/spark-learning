{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_DRIVER_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYSPARK_PYTHON=/Users/c11309a/.local/share/rtx/installs/python/3.10/bin/python\n",
      "PYTHONPATH=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/pyspark.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3/python/lib/*.zip:\n",
      "SPARK_HOME=/Users/c11309a/Tools/spark-3.3.4-bin-hadoop3\n",
      "PYTHONUNBUFFERED=1\n",
      "PYTHONIOENCODING=utf-8\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING=1\n"
     ]
    }
   ],
   "source": [
    "# Check env vars\n",
    "!env | grep -e \"SPARK\" -e \"PYTHON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/28 09:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/28 09:33:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/12/28 09:33:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.150:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>learn_dataframes</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10e8bd4b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "            SparkSession.builder.appName(\"learn_dataframes\")\n",
    "                .master(\"local[4]\")\n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common\n",
    "from pyspark.sql.functions import udf, col, to_date\n",
    "from pyspark.sql.types import StructType, StructField, DataType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------+-----+\n",
      "|Id |StartDate |EndDate   |Type    |Name |\n",
      "+---+----------+----------+--------+-----+\n",
      "|1  |2023-01-01|2023-01-07|Vacation|John |\n",
      "|1  |2023-02-09|2023-02-11|Sick    |John |\n",
      "|2  |2023-03-01|2023-03-07|Vacation|Smith|\n",
      "|2  |2023-04-15|2023-04-20|Sick    |Smith|\n",
      "|3  |2023-05-10|2023-05-15|Vacation|Mark |\n",
      "|3  |2023-06-01|2023-06-07|Sick    |Mark |\n",
      "|4  |2023-07-01|2023-07-07|Vacation|David|\n",
      "|4  |2023-08-01|2023-08-07|Sick    |David|\n",
      "|5  |2023-09-01|2023-09-07|Vacation|Paul |\n",
      "|5  |2023-10-01|2023-10-07|Sick    |Paul |\n",
      "|3  |2023-11-01|2023-11-07|Vacation|Mark |\n",
      "|3  |2023-12-01|2023-12-07|Sick    |Mark |\n",
      "|4  |2024-01-01|2024-01-07|Vacation|David|\n",
      "|2  |2024-02-01|2024-02-07|Sick    |Smith|\n",
      "|1  |2024-03-01|2024-03-07|Vacation|John |\n",
      "|3  |2024-04-01|2024-04-07|Sick    |Mark |\n",
      "|2  |2024-05-01|2024-05-07|Vacation|Smith|\n",
      "+---+----------+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from data/employees.csv and data/employees_fto.csv and join them on id\n",
    "employees_df = spark.read.csv(\"data/employees.csv\", header=True)\n",
    "employees_fto_df = spark.read.csv(\"data/employees_fto.csv\", header=True)\n",
    "\n",
    "join_df = (\n",
    "    employees_fto_df.join(employees_df, on=\"Id\", how=\"left\")\n",
    "        .drop('salary')\n",
    "        .withColumn(\"StartDate\", to_date(col(\"StartDate\"), \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"EndDate\", to_date(col(\"EndDate\"), \"yyyy-MM-dd\"))\n",
    ")\n",
    "join_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- StartDate: date (nullable = true)\n",
      " |-- EndDate: date (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "join_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------+-----+-------+\n",
      "|Id |StartDate |EndDate   |Type    |Name |DaysOff|\n",
      "+---+----------+----------+--------+-----+-------+\n",
      "|1  |2023-01-01|2023-01-07|Vacation|John |6      |\n",
      "|1  |2023-02-09|2023-02-11|Sick    |John |2      |\n",
      "|2  |2023-03-01|2023-03-07|Vacation|Smith|6      |\n",
      "|2  |2023-04-15|2023-04-20|Sick    |Smith|5      |\n",
      "|3  |2023-05-10|2023-05-15|Vacation|Mark |5      |\n",
      "|3  |2023-06-01|2023-06-07|Sick    |Mark |6      |\n",
      "|4  |2023-07-01|2023-07-07|Vacation|David|6      |\n",
      "|4  |2023-08-01|2023-08-07|Sick    |David|6      |\n",
      "|5  |2023-09-01|2023-09-07|Vacation|Paul |6      |\n",
      "|5  |2023-10-01|2023-10-07|Sick    |Paul |6      |\n",
      "|3  |2023-11-01|2023-11-07|Vacation|Mark |6      |\n",
      "|3  |2023-12-01|2023-12-07|Sick    |Mark |6      |\n",
      "|4  |2024-01-01|2024-01-07|Vacation|David|6      |\n",
      "|2  |2024-02-01|2024-02-07|Sick    |Smith|6      |\n",
      "|1  |2024-03-01|2024-03-07|Vacation|John |6      |\n",
      "|3  |2024-04-01|2024-04-07|Sick    |Mark |6      |\n",
      "|2  |2024-05-01|2024-05-07|Vacation|Smith|6      |\n",
      "+---+----------+----------+--------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate days off based on start and end dates\n",
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "join_df = join_df.withColumn(\"DaysOff\", datediff(col(\"EndDate\"), col(\"StartDate\")))\n",
    "join_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+-----+\n",
      "|Name |Type    |SumDaysOff|Count|\n",
      "+-----+--------+----------+-----+\n",
      "|David|Sick    |6         |1    |\n",
      "|David|Vacation|12        |2    |\n",
      "|John |Sick    |2         |1    |\n",
      "|John |Vacation|12        |2    |\n",
      "|Mark |Sick    |18        |3    |\n",
      "|Mark |Vacation|11        |2    |\n",
      "|Paul |Sick    |6         |1    |\n",
      "|Paul |Vacation|6         |1    |\n",
      "|Smith|Sick    |11        |2    |\n",
      "|Smith|Vacation|12        |2    |\n",
      "+-----+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate total days off by Name and Type\n",
    "from pyspark.sql.functions import sum, count\n",
    "\n",
    "summary_df = (\n",
    "    join_df.groupBy(\"Name\", \"Type\")\n",
    "        .agg(\n",
    "            sum(\"DaysOff\").alias(\"SumDaysOff\"), \n",
    "            count(\"*\").alias(\"Count\")\n",
    "        )\n",
    ").sort(\"Name\", \"Type\")\n",
    "\n",
    "summary_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----------+-----+\n",
      "|Name |Type    |SumDaysOff|Count|\n",
      "+-----+--------+----------+-----+\n",
      "|David|Sick    |6         |1    |\n",
      "|David|Vacation|12        |2    |\n",
      "|John |Sick    |2         |1    |\n",
      "|John |Vacation|12        |2    |\n",
      "|Mark |Sick    |18        |3    |\n",
      "|Mark |Vacation|11        |2    |\n",
      "|Paul |Sick    |6         |1    |\n",
      "|Paul |Vacation|6         |1    |\n",
      "|Smith|Sick    |11        |2    |\n",
      "|Smith|Vacation|12        |2    |\n",
      "+-----+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to join data/employees.csv and data/employees_fto.csv and calculate total days off by Name and Type\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "employees_fto_df.createOrReplaceTempView(\"employees_fto\")\n",
    "\n",
    "summary_df = spark.sql(\"\"\"\n",
    "    SELECT Name, Type, SUM(DATEDIFF(EndDate, StartDate)) AS SumDaysOff, COUNT(8) AS Count\n",
    "    FROM employees_fto\n",
    "    JOIN employees ON employees_fto.Id = employees.Id\n",
    "    GROUP BY Name, Type\n",
    "    ORDER BY Name, Type\n",
    "\"\"\")\n",
    "\n",
    "summary_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
